{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Graph Anomaly Prediction\n",
        "\n",
        "---\n",
        "On Yelp Reviews Dataset and Elliptic dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qpV4kHEwJ9TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Imports\n",
        "All relevant project libraries and utilities for the notebook are installed and imported here."
      ],
      "metadata": {
        "id": "dJRugET9JcHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GhQTg7ubLzC"
      },
      "outputs": [],
      "source": [
        "!pip install spektral"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --ignore-installed --upgrade tensorflow"
      ],
      "metadata": {
        "id": "dPaEQ8ZFbTqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pickle\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import polynomial\n",
        "from scipy.special import comb\n",
        "import scipy.sparse as ss"
      ],
      "metadata": {
        "id": "rAuHI2bObU4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bebb3a-b60b-40b6-9a33-f9de46ae7605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from tensorflow.keras import backend as KB\n",
        "from tensorflow.experimental import Optional as OptTensor\n",
        "from tensorflow.python.ops.numpy_ops import np_config"
      ],
      "metadata": {
        "id": "3gGP8Bm_AIWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras.layers import Dropout, Input, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "IJOaGGwMBUt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spektral is a Python Library which is used for Graph Deep Learning based on Keras and TensorFlow. We have used this as an alternative to PyG Library."
      ],
      "metadata": {
        "id": "O-lGKATpu01_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spektral as spktrl\n",
        "from spektral.data import SingleLoader\n",
        "from spektral.data import Dataset, Graph\n",
        "from spektral.data.dataset import Dataset\n",
        "from spektral.layers import MessagePassing\n",
        "from spektral.layers import ops\n",
        "from spektral.layers.convolutional.conv import Conv\n",
        "from spektral.utils import normalized_laplacian, rescale_laplacian, laplacian, degree_matrix"
      ],
      "metadata": {
        "id": "dblKuk4HbxaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.datasets.citation import Citation\n",
        "from spektral.layers import ChebConv\n",
        "from spektral.transforms import LayerPreprocess\n",
        "from spektral.layers import GATConv"
      ],
      "metadata": {
        "id": "RZmncEFjBwWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_config.enable_numpy_behavior()"
      ],
      "metadata": {
        "id": "PJlWQzFnAq3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xktG83OWxk9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Data Importation"
      ],
      "metadata": {
        "id": "kG_J1B_3K96j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Upload datasets to Drive and mount Drive\n",
        "2. Unzip the datasets  \n",
        "3. Uncomment graph if testing this model on respective Elliptic dataset or Yelp dataset\n",
        "4. Give path to the dataset"
      ],
      "metadata": {
        "id": "JkU6mljJ8LK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "kn6_UBy58jkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd021805-6e2a-485a-c697-12677692c230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip gdrive/My\\ Drive/ellipticFraud.zip"
      ],
      "metadata": {
        "id": "UMvYTfqa8oTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/yelpFraud.zip"
      ],
      "metadata": {
        "id": "6PamY2y1e8WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbd4a7b-3655-45cd-e300-d2a6254c4381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gdrive/My Drive/yelpFraud.zip\n",
            "  inflating: yelpFraud.dat           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# graph=pickle.load(open(\"/content/ellipticFraud.dat\",\"rb\"))"
      ],
      "metadata": {
        "id": "Ku-hjDeW9YKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = pickle.load(open(\"/content/yelpFraud.dat\", \"rb\"))"
      ],
      "metadata": {
        "id": "c1n8engbAxEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph converted to spektral data object for processing in keras and tensor flow, \n",
        "# Its a graph container. \n",
        "# label is either anomolous or normal. \n",
        "class MyDataset(Dataset):\n",
        "\n",
        "   def __init__(self, **kwargs):\n",
        "      \n",
        "        self.mask_tr = self.mask_te = self.mask_va = None\n",
        "        self.num_nodes= self.num_features=None\n",
        "        super().__init__(**kwargs)\n",
        "   def read(self):\n",
        "        g=graph  \n",
        "        x = g.x\n",
        "        a = g.a\n",
        "\n",
        "        y = g.y.reshape(-1,1)\n",
        "        anomaly = (g.y == 1)\n",
        "        normal = (g.y == 0)\n",
        "        label = (anomaly, normal)\n",
        "        \n",
        "        y=np.stack((normal, anomaly))\n",
        "        y=y*1\n",
        "        y=np.transpose(y)\n",
        "        self.num_nodes= a.shape[0]\n",
        "        self.num_features= x.shape[1]\n",
        "        \n",
        "        self.mask_tr = 1*g.train_mask\n",
        "        self.mask_te = 1*g.test_mask\n",
        "        self.mask_va = 1*g.val_mask\n",
        "\n",
        "        return [Graph(x=x, a=a, y=y)]\n",
        "        "
      ],
      "metadata": {
        "id": "9WtzPnekBMA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= MyDataset()\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "_TqwCilABNQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7088a1dd-5b0f-4ea6-dd3b-869e8b1a5c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(n_nodes=45954, n_node_features=32, n_edge_features=None, n_labels=2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for processing on single graph, single data loader is used. \n",
        "#It internally converts sparse arrays and numpy arrays to keras Tensor. \n",
        "\n",
        "loader=SingleLoader(dataset)\n",
        "inputs= loader.load()\n",
        "inputs"
      ],
      "metadata": {
        "id": "z40O1hAyBQi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33df1215-fdb5-4854-eb9e-d32387e8a753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RepeatDataset element_spec=((TensorSpec(shape=(45954, 32), dtype=tf.float32, name=None), SparseTensorSpec(TensorShape([45954, 45954]), tf.float32)), TensorSpec(shape=(45954, 2), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We converted the binary masks to sample weights so that we can compute the\n",
        "# average loss over the nodes (following original implementation by\n",
        "# Kipf & Welling)\n",
        "\n",
        "def mask_to_weights(mask):\n",
        "    return mask / np.count_nonzero(mask)\n",
        "\n",
        "weights_tr, weights_va, weights_te = (\n",
        "    mask_to_weights(mask)\n",
        "    for mask in (dataset.mask_tr, dataset.mask_va, dataset.mask_te)\n",
        ")"
      ],
      "metadata": {
        "id": "qbj_YJKCB6LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Bernstein Convolution\n",
        "Bernstein polynomial makes a filter\n",
        "\n",
        "Call(): The messages are gathered from the neighbouring nodes. The number of neighbours depends on  the degree of polynomial. If degree is 'K', the messages are gathered from 'K' hop neighbours.\n",
        "\n",
        "Preprocess():\n",
        "A function preprocess is used to preprocess the input adjacency matrix using laplacian functions. \n",
        "\n",
        "get_bern_coeff():\n",
        "The filters are paramterized using bernstien polynomial co-efficients, the values range  between 0-1.\n"
      ],
      "metadata": {
        "id": "24egpdgbLaAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BernConv(Conv):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        K=1,\n",
        "        activation=None,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.channels = channels\n",
        "        self.K = K\n",
        "\n",
        "        # initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
        "        # self.wazn = (initializer(shape=(K+1, 1)))\n",
        "        # # self.mask= tf.keras.backend.variable(self.wazn, trainable=True)\n",
        "        # self.mask= tf.keras.backend.variable(self.wazn)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[0][-1]\n",
        "        \n",
        "        # learnable weight matrix, Parameters are learned for high pass and low pass filters\n",
        "        self.kernel = self.add_weight( \n",
        "            shape=(self.K, input_dim, self.channels),\n",
        "            initializer=self.kernel_initializer,\n",
        "            name=\"kernel\",\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(self.channels,),\n",
        "                initializer=self.bias_initializer,\n",
        "                name=\"bias\",\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "            )\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "\n",
        "        # takes adjacency matrix and feature vector as input, \n",
        "        # adjacency matrix is preprocessed to laplacian and normalized laplacian   \n",
        "        x, a = inputs  \n",
        "        \n",
        "        # messages are calculated \n",
        "        Bx_0 = ops.modal_dot(a, x)  \n",
        "        Bx = [Bx_0]\n",
        "        Bx_next = Bx_0\n",
        "        out = tf.keras.backend.zeros_like(x)\n",
        "        for i in range(self.K):\n",
        "            bx_next = ops.modal_dot(a, Bx_next)\n",
        "            Bx_next = KB.dot(Bx_next, self.kernel[i])\n",
        "            Bx.append(Bx_next)    # depending on degree of polynomial, messages are gathered. \n",
        "        bern_coeff =  BernConv.get_bern_coeff(self.K)\n",
        "\n",
        "        # the filters are paramterized using bernstien polynomial co-efficients, ranging between 0-1\n",
        "        for k in range(0, self.K + 1):\n",
        "            coeff = bern_coeff[k]\n",
        "            basis = Bx[0] * coeff[0]\n",
        "            for i in range(1, self.K + 1):\n",
        "                basis += Bx[i] * coeff[i]\n",
        "            out += basis\n",
        "        return out\n",
        "\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return {\"channels\": self.channels, \"K\": self.K}\n",
        "\n",
        "    # part of conv layer, used to preprocess the input adjacency matrix depending on the use case\n",
        "    @staticmethod\n",
        "    def preprocess(a):\n",
        "        a = normalized_laplacian(a)\n",
        "        a = rescale_laplacian(a)\n",
        "        return a\n",
        "\n",
        "    # used to calculate bern co-efficients\n",
        "    @staticmethod\n",
        "    def get_bern_coeff(degree):\n",
        "\n",
        "        def Bernstein(de, i):\n",
        "            #coefficients = [0, ] * i + [math.comb(de, i)]\n",
        "            coefficients = [0, ] * i + [comb(de, i)]\n",
        "            \n",
        "            first_term = polynomial.polynomial.Polynomial(coefficients)\n",
        "            second_term = polynomial.polynomial.Polynomial([1, -1]) ** (de - i)\n",
        "            return first_term * second_term\n",
        "\n",
        "        out = []\n",
        "\n",
        "        for i in range(degree + 1):\n",
        "            out.append(Bernstein(degree, i).coef)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "h3MzlIc9Cl3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Train Model\n",
        "\n",
        "Model Parameters"
      ],
      "metadata": {
        "id": "jLLV5WtjDuph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# paramters for the network \n",
        "\n",
        "channels = 64  # Number of channels in the first layer\n",
        "K = 5  # Max degree of the Bernstien polynomials\n",
        "dropout = 0.3  # Dropout rate for the features\n",
        "l2_reg = 2.5e-4  # L2 regularization rate\n",
        "learning_rate = 5e-4  # Learning rate\n",
        "epochs = 2000  # Number of training epochs\n",
        "patience = 200  # Patience for early stopping\n",
        "a_dtype = dataset[0].a.dtype  \n",
        "\n",
        "N = dataset.n_nodes  # Number of nodes in the graph\n",
        "F = dataset.n_node_features  # Original size of node features\n",
        "n_out = dataset.n_labels  # Number of classes"
      ],
      "metadata": {
        "id": "bNWsZy5oB68K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition"
      ],
      "metadata": {
        "id": "yDTaogBUEHRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_in = Input(shape=(F,)) # F is the dimenion of features , in case of yelp its 32\n",
        "a_in = Input((N,), sparse=True, dtype=a_dtype) # N is the number of nodes // custom input to genrate layers of Model \n",
        "\n",
        "\n",
        "den1=Dense(channels, activation=\"relu\")(x_in) # linear transformation from x_in to channels with relu activation . \n",
        "den2=Dense(channels)(den1) # another linear transformation with same input and output channels. \n",
        "\n",
        "\n",
        "# Two separate filters, one as high pass filter, other as low pass filter. Parameters are learned by the model \n",
        "conv1=BernConv(channels, k =5)\n",
        "conv2=BernConv(channels, k=5)\n",
        "\n",
        "# conv1=ChebConv(channels, k=5)\n",
        "# conv2=ChebConv(channels, k=5)\n",
        "h_list = []\n",
        "\n",
        "h1 = conv1([den2, a_in])\n",
        "h_list.append(h1)\n",
        "\n",
        "h2 =conv2([den2, a_in])\n",
        "h_list.append(h2)\n",
        "# Filters are stacked together. \n",
        "h_filters = keras.backend.stack(h_list, axis=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ha3rIP99B-RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining Signals Adaptively"
      ],
      "metadata": {
        "id": "Tnll6rSHN_JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lambda function used to conduct operations between two keras layers. \n",
        "def custom_layer(inputs):\n",
        "  a, b =inputs\n",
        "  res=a[:, 0, :] * b[:, 0]\n",
        "  for i in range(1, 2): # range is number of filters , here we are using 2 filters\n",
        "      res += a[:, i, :] * b[:, i]\n",
        "  return res"
      ],
      "metadata": {
        "id": "mJjUHuwOkBmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Constraint"
      ],
      "metadata": {
        "id": "C_6wKkUYOl-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Node Level Attention. where h_filters_proj and x_proj indicate weight matricies transformation of the filters and input feature vectors,  transformed linearly. \n",
        "\n",
        "h_filters_proj = Dense(channels, activation=\"tanh\")(h_filters)\n",
        "x_proj=Dense(channels, activation=\"tanh\") (den2)\n",
        "x_proj = tf.keras.backend.expand_dims(x_proj,axis=-1)\n",
        "\n",
        "\n",
        "score_logit = tf.keras.backend.batch_dot(h_filters_proj, x_proj)\n",
        "softmax =tf.keras.layers.Softmax(axis=1)\n",
        "\n",
        "score = softmax(score_logit)\n",
        "\n",
        "lmbda = tf.keras.layers.Lambda(custom_layer)\n",
        "res=lmbda([h_filters , score])\n",
        "\n",
        "# after calculating the score dropout is used with rate 0.3\n",
        "do1= Dropout(rate=0.1)(res)\n",
        "den3=Dense(n_out, activation=\"softmax\")(do1)"
      ],
      "metadata": {
        "id": "D3cnn2tesG4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "IsNIO-uKENAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import AUC\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "model = Model(inputs=[x_in, a_in], outputs=den3)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "def pr_auc(y_true, y_pred):\n",
        "    return tf.py_function(average_precision_score, (y_true, y_pred), tf.float64)\n",
        "    \n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=CategoricalCrossentropy(reduction=\"sum\"),  # To compute mean\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        "    # metrics=[roc_auc,pr_auc]\n",
        "    metrics=[AUC(name='ROC_auc'),pr_auc]) # to compute auc-roc and pr score. \n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Th-0YvvHCRCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f04c220-edb9-4e25-8973-b76223324f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 32)]         0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           2112        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 64)           4160        ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 45954)]      0           []                               \n",
            "                                                                                                  \n",
            " bern_conv_2 (BernConv)         (None, 64)           4160        ['dense_3[0][0]',                \n",
            "                                                                  'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " bern_conv_3 (BernConv)         (None, 64)           4160        ['dense_3[0][0]',                \n",
            "                                                                  'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " tf.stack_1 (TFOpLambda)        (None, 2, 64)        0           ['bern_conv_2[0][0]',            \n",
            "                                                                  'bern_conv_3[0][0]']            \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           4160        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 2, 64)        4160        ['tf.stack_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda)    (None, 64, 1)        0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLambda)  (None, 2, 1)         0           ['dense_4[0][0]',                \n",
            "                                                                  'tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " softmax (Softmax)              (None, 2, 1)         0           ['tf.linalg.matmul[0][0]']       \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 64)           0           ['tf.stack_1[0][0]',             \n",
            "                                                                  'softmax[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 64)           0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 2)            130         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,042\n",
            "Trainable params: 23,042\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "RtRnXV3CEQuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loader is used to feed data into the model\n",
        "loader_tr = SingleLoader(dataset, sample_weights=weights_tr) \n",
        "loader_va = SingleLoader(dataset, sample_weights=weights_va)"
      ],
      "metadata": {
        "id": "U9Sz54rztBys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    loader_tr.load(),\n",
        "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
        "    validation_data=loader_va.load(),\n",
        "    validation_steps=loader_va.steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        ")"
      ],
      "metadata": {
        "id": "Ac_6BOJOCR76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f6fdaa-37e6-4749-d971-17849e237f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4645 - ROC_auc: 0.8694 - pr_auc: 0.5349 - accuracy: 0.8423 - val_loss: 0.4189 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1974/2000\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.4607 - ROC_auc: 0.8689 - pr_auc: 0.5339 - accuracy: 0.8422 - val_loss: 0.4165 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5558 - val_accuracy: 0.8562\n",
            "Epoch 1975/2000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.4637 - ROC_auc: 0.8666 - pr_auc: 0.5317 - accuracy: 0.8403 - val_loss: 0.4189 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1976/2000\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.4635 - ROC_auc: 0.8684 - pr_auc: 0.5321 - accuracy: 0.8411 - val_loss: 0.4179 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1977/2000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.4667 - ROC_auc: 0.8663 - pr_auc: 0.5301 - accuracy: 0.8407 - val_loss: 0.4171 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5558 - val_accuracy: 0.8562\n",
            "Epoch 1978/2000\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.4648 - ROC_auc: 0.8680 - pr_auc: 0.5319 - accuracy: 0.8388 - val_loss: 0.4186 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1979/2000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.4615 - ROC_auc: 0.8684 - pr_auc: 0.5335 - accuracy: 0.8413 - val_loss: 0.4179 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1980/2000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.4618 - ROC_auc: 0.8686 - pr_auc: 0.5340 - accuracy: 0.8429 - val_loss: 0.4167 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5559 - val_accuracy: 0.8562\n",
            "Epoch 1981/2000\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.4606 - ROC_auc: 0.8680 - pr_auc: 0.5339 - accuracy: 0.8398 - val_loss: 0.4205 - val_ROC_auc: 0.8793 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1982/2000\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.4676 - ROC_auc: 0.8674 - pr_auc: 0.5299 - accuracy: 0.8421 - val_loss: 0.4175 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1983/2000\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.4627 - ROC_auc: 0.8684 - pr_auc: 0.5332 - accuracy: 0.8387 - val_loss: 0.4172 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1984/2000\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.4639 - ROC_auc: 0.8677 - pr_auc: 0.5336 - accuracy: 0.8409 - val_loss: 0.4191 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1985/2000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4643 - ROC_auc: 0.8678 - pr_auc: 0.5317 - accuracy: 0.8422 - val_loss: 0.4176 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1986/2000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4639 - ROC_auc: 0.8680 - pr_auc: 0.5333 - accuracy: 0.8409 - val_loss: 0.4173 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5559 - val_accuracy: 0.8564\n",
            "Epoch 1987/2000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4576 - ROC_auc: 0.8699 - pr_auc: 0.5367 - accuracy: 0.8415 - val_loss: 0.4192 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1988/2000\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.4607 - ROC_auc: 0.8698 - pr_auc: 0.5343 - accuracy: 0.8423 - val_loss: 0.4171 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1989/2000\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.4647 - ROC_auc: 0.8678 - pr_auc: 0.5324 - accuracy: 0.8406 - val_loss: 0.4179 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1990/2000\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4654 - ROC_auc: 0.8675 - pr_auc: 0.5315 - accuracy: 0.8411 - val_loss: 0.4183 - val_ROC_auc: 0.8794 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1991/2000\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.4639 - ROC_auc: 0.8683 - pr_auc: 0.5322 - accuracy: 0.8424 - val_loss: 0.4172 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1992/2000\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.4607 - ROC_auc: 0.8691 - pr_auc: 0.5342 - accuracy: 0.8411 - val_loss: 0.4165 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1993/2000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.4654 - ROC_auc: 0.8664 - pr_auc: 0.5296 - accuracy: 0.8379 - val_loss: 0.4207 - val_ROC_auc: 0.8793 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1994/2000\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.4657 - ROC_auc: 0.8680 - pr_auc: 0.5303 - accuracy: 0.8429 - val_loss: 0.4156 - val_ROC_auc: 0.8796 - val_pr_auc: 0.5560 - val_accuracy: 0.8562\n",
            "Epoch 1995/2000\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.4604 - ROC_auc: 0.8686 - pr_auc: 0.5350 - accuracy: 0.8384 - val_loss: 0.4182 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1996/2000\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.4610 - ROC_auc: 0.8693 - pr_auc: 0.5331 - accuracy: 0.8409 - val_loss: 0.4190 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1997/2000\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4641 - ROC_auc: 0.8685 - pr_auc: 0.5318 - accuracy: 0.8422 - val_loss: 0.4157 - val_ROC_auc: 0.8796 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1998/2000\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.4628 - ROC_auc: 0.8671 - pr_auc: 0.5332 - accuracy: 0.8383 - val_loss: 0.4207 - val_ROC_auc: 0.8793 - val_pr_auc: 0.5560 - val_accuracy: 0.8564\n",
            "Epoch 1999/2000\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.4637 - ROC_auc: 0.8690 - pr_auc: 0.5324 - accuracy: 0.8428 - val_loss: 0.4173 - val_ROC_auc: 0.8795 - val_pr_auc: 0.5561 - val_accuracy: 0.8564\n",
            "Epoch 2000/2000\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.4625 - ROC_auc: 0.8682 - pr_auc: 0.5327 - accuracy: 0.8410 - val_loss: 0.4157 - val_ROC_auc: 0.8796 - val_pr_auc: 0.5560 - val_accuracy: 0.8562\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd040071730>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IV. Model Evaluation\n",
        "\n",
        "Accuracy and Loss in Model predictions"
      ],
      "metadata": {
        "id": "kNWzA_vdEefZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating model.\")\n",
        "loader_te = SingleLoader(dataset, sample_weights=weights_te)\n",
        "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
        "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))"
      ],
      "metadata": {
        "id": "rZj0aIl6CZO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a85e47-5186-4aca-e8c2-fc74a166125b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model.\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.4108 - ROC_auc: 0.8796 - pr_auc: 0.5560 - accuracy: 0.8574\n",
            "Done.\n",
            "Test loss: 0.41082948446273804\n",
            "Test accuracy: 0.8795819282531738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIugb5GX0Vc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}